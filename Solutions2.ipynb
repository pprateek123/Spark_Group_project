{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"solutions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum , col ,length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets \n",
    "\n",
    "data_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/revised_final_data1.csv\"  \n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "data_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/goods_classification.csv\"  \n",
    "df_goods = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "data_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/country_classification.csv\"  \n",
    "df_country = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "data_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/services_classification.csv\"  \n",
    "df_services = spark.read.csv(data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time_ref: date (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[time_ref: date, account: string, code: string, country_code: string, product_type: string, value: int, status: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('value', col('value').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+------------+------------+-------------+------+\n",
      "|  time_ref|account|code|country_code|product_type|        value|status|\n",
      "+----------+-------+----+------------+------------+-------------+------+\n",
      "|2023-06-30|Exports|  00|          AE|       Goods| 2.82028909E8|     F|\n",
      "|2023-06-30|Exports|  00|          AG|       Goods|     351919.0|     F|\n",
      "|2023-06-30|Exports|  00|          AI|       Goods|      84762.0|     F|\n",
      "|2023-06-30|Exports|  00|          AL|       Goods|       3463.0|     F|\n",
      "|2023-06-30|Exports|  00|          AM|       Goods|     679586.0|     F|\n",
      "|2023-06-30|Exports|  00|          AO|       Goods|     464583.0|     F|\n",
      "|2023-06-30|Exports|  00|          AR|       Goods|    5943055.0|     F|\n",
      "|2023-06-30|Exports|  00|          AS|       Goods|  1.1622992E7|     F|\n",
      "|2023-06-30|Exports|  00|          AT|       Goods|  1.1202334E7|     F|\n",
      "|2023-06-30|Exports|  00|          AU|       Goods|2.272665701E9|     F|\n",
      "|2023-06-30|Exports|  00|          AW|       Goods|     362396.0|     F|\n",
      "|2023-06-30|Exports|  00|          AZ|       Goods|    5816857.0|     F|\n",
      "|2023-06-30|Exports|  00|          BB|       Goods|    8464598.0|     F|\n",
      "|2023-06-30|Exports|  00|          BD|       Goods| 1.77083931E8|     F|\n",
      "|2023-06-30|Exports|  00|          BE|       Goods|  9.4312051E7|     F|\n",
      "|2023-06-30|Exports|  00|          BF|       Goods|      36757.0|     F|\n",
      "|2023-06-30|Exports|  00|          BG|       Goods|    3697081.0|     F|\n",
      "|2023-06-30|Exports|  00|          BH|       Goods|   2.272435E7|     F|\n",
      "|2023-06-30|Exports|  00|          BJ|       Goods|     507511.0|     F|\n",
      "|2023-06-30|Exports|  00|          BM|       Goods|     529761.0|     F|\n",
      "+----------+-------+----+------------+------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Using a UDF, calculate the total export value for each product type in the \"revised_csv\" file for the June 2023 quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 08:59:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ExportValueByProductType\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ExportValueByProductType\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"revised_csv\" data\n",
    "revised_df = spark.read.csv(\"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/country_classification.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+------------+------------+-------------+------+\n",
      "|  time_ref|account|code|country_code|product_type|        value|status|\n",
      "+----------+-------+----+------------+------------+-------------+------+\n",
      "|2023-06-30|Exports|  00|          AE|       Goods| 2.82028909E8|     F|\n",
      "|2023-06-30|Exports|  00|          AG|       Goods|     351919.0|     F|\n",
      "|2023-06-30|Exports|  00|          AI|       Goods|      84762.0|     F|\n",
      "|2023-06-30|Exports|  00|          AL|       Goods|       3463.0|     F|\n",
      "|2023-06-30|Exports|  00|          AM|       Goods|     679586.0|     F|\n",
      "|2023-06-30|Exports|  00|          AO|       Goods|     464583.0|     F|\n",
      "|2023-06-30|Exports|  00|          AR|       Goods|    5943055.0|     F|\n",
      "|2023-06-30|Exports|  00|          AS|       Goods|  1.1622992E7|     F|\n",
      "|2023-06-30|Exports|  00|          AT|       Goods|  1.1202334E7|     F|\n",
      "|2023-06-30|Exports|  00|          AU|       Goods|2.272665701E9|     F|\n",
      "|2023-06-30|Exports|  00|          AW|       Goods|     362396.0|     F|\n",
      "|2023-06-30|Exports|  00|          AZ|       Goods|    5816857.0|     F|\n",
      "|2023-06-30|Exports|  00|          BB|       Goods|    8464598.0|     F|\n",
      "|2023-06-30|Exports|  00|          BD|       Goods| 1.77083931E8|     F|\n",
      "|2023-06-30|Exports|  00|          BE|       Goods|  9.4312051E7|     F|\n",
      "|2023-06-30|Exports|  00|          BF|       Goods|      36757.0|     F|\n",
      "|2023-06-30|Exports|  00|          BG|       Goods|    3697081.0|     F|\n",
      "|2023-06-30|Exports|  00|          BH|       Goods|   2.272435E7|     F|\n",
      "|2023-06-30|Exports|  00|          BJ|       Goods|     507511.0|     F|\n",
      "|2023-06-30|Exports|  00|          BM|       Goods|     529761.0|     F|\n",
      "+----------+-------+----+------------+------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to calculate the total export value, handling non-numeric values\n",
    "def calculate_total_export_value(values):\n",
    "    # Initialize a sum variable\n",
    "    total = 0.0\n",
    "    \n",
    "    # Iterate through the values and sum them if they are numeric\n",
    "    for val in values:\n",
    "        try:\n",
    "            total += float(val)\n",
    "        except (ValueError, TypeError):\n",
    "            pass  # Skip non-numeric or null values\n",
    "    \n",
    "    return total\n",
    "\n",
    "# Register the UDF\n",
    "calculate_total_export_udf = udf(calculate_total_export_value, DoubleType())\n",
    "\n",
    "# Calculate the total export value for each product type in the June 2023 quarter\n",
    "result_df = df.filter(df.time_ref == \"2023-06-30\") \\\n",
    "    .groupBy(\"product_type\") \\\n",
    "    .agg(calculate_total_export_udf(F.collect_list(\"value\")).alias(\"total_export_value\"))\n",
    "\n",
    "# Format the numeric values with 1 decimal place\n",
    "result_df = result_df.withColumn(\"total_export_value\", F.format_number(\"total_export_value\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|product_type|total_export_value|\n",
      "+------------+------------------+\n",
      "|    Services|  38,639,599,055.0|\n",
      "|       Goods| 152,435,617,373.0|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Generate a pivot table that shows the total import values of Services for the top 4 importing countries in June 2023, with countries as rows and the \"Transportation\" service categories as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 09:06:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ImportServicePivot\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant datasets\n",
    "revised_csv_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/revised_final_data.csv\"\n",
    "service_csv_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/services_classification.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the revised_csv and service_csv\n",
    "revised_df = spark.read.csv(revised_csv_path, header=True, inferSchema=True)\n",
    "service_df = spark.read.csv(service_csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset for imports in June 2023-06-30\n",
    "import_df = revised_df.filter((col(\"account\") == \"Imports\") & (col(\"time_ref\") == \"2023-06-30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the import_df with service_df on the \"code\" column\n",
    "joined_df = import_df.join(service_df, import_df[\"code\"] == service_df[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by country_code and service_label, and sum the import values\n",
    "grouped_df = joined_df.groupBy(\"country_code\", \"service_label\").sum(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Rank the countries by total import values in descending order\n",
    "ranked_df = grouped_df.orderBy(col(\"sum(value)\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top 5 importing countries\n",
    "top_countries = ranked_df.select(\"country_code\").limit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to create the desired pivot table\n",
    "pivot_table = ranked_df \\\n",
    "    .join(top_countries, [\"country_code\"], \"inner\") \\\n",
    "    .groupBy(\"country_code\").pivot(\"service_label\", [\"Services\", \"Transportation\"]) \\\n",
    "    .sum(\"sum(value)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------+\n",
      "|country_code|     Services|Transportation|\n",
      "+------------+-------------+--------------+\n",
      "|          AU|1.881982847E9|  2.49329771E8|\n",
      "|          US|1.018642986E9|   7.6008244E7|\n",
      "|          XX| 6.81217569E8|   6.3025441E7|\n",
      "|          SG| 9.09750687E8|  3.26007453E8|\n",
      "+------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the resulting pivot table\n",
    "pivot_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.8 Calculate the total import value of \"Services\" for each country in June 2023-06-30, and rank the countries in descending order of import value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ImportServicesValue\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant datasets\n",
    "revised_csv_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/revised_final_data1.csv\"\n",
    "service_csv_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/services_classification.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the revised_csv and service_csv\n",
    "revised_df = spark.read.csv(revised_csv_path, header=True, inferSchema=True)\n",
    "service_df = spark.read.csv(service_csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the 'revised_df' DataFrame contains your data\n",
    "import_df = revised_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for imports in June 2023-06-30 and 'Services'\n",
    "import_df = import_df.filter((col(\"account\") == \"Imports\") & (col(\"time_ref\") == \"2023-06-30\") & (col(\"product_type\") == \"Services\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Group by 'country_code' and calculate the sum of 'value' for each group\n",
    "import_df = import_df.groupBy(\"country_code\").agg(sum(\"value\").alias(\"Total Import Value\"))\n",
    "\n",
    "# Rank the countries in descending order of import value\n",
    "import_df = import_df.orderBy(col(\"Total Import Value\").desc())\n",
    "\n",
    "# Rename the columns as per your desired output\n",
    "import_df = import_df.withColumnRenamed(\"country_code\", \"Country Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Country Code|Total Import Value|\n",
      "+------------+------------------+\n",
      "|          AU|     6.571340755E9|\n",
      "|          US|      3.46553267E9|\n",
      "|          XX|      2.03107177E9|\n",
      "|          SG|     1.831464711E9|\n",
      "|          CH|     1.136472799E9|\n",
      "|          GB|     1.116098864E9|\n",
      "|          DK|     1.080059039E9|\n",
      "|          CN|      6.56124353E8|\n",
      "|          IE|      3.62344309E8|\n",
      "|          IN|      3.32702047E8|\n",
      "|          CK|      2.83971901E8|\n",
      "|          JP|      2.45157897E8|\n",
      "|          BM|      2.13337641E8|\n",
      "|          PH|      1.95167659E8|\n",
      "|          WS|      1.93461138E8|\n",
      "|          FR|      1.78350741E8|\n",
      "|          ID|      1.72441524E8|\n",
      "|          HK|      1.69274501E8|\n",
      "|          DE|      1.52909324E8|\n",
      "|          CA|      1.51301869E8|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the resulting DataFrame\n",
    "import_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Find the top 3 countries with the highest average export value of \"Goods\" for each month in the updated time_ref (2023-06-30). Use window functions to calculate the average and rank the countries accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, avg, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 10:27:17 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1623)\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/09/06 10:27:18 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
      "\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:556)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:556)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Initialize SparkSession\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mTop3CountriesByAvgExport\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    478\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    513\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    198\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[1;32m    203\u001b[0m         sparkHome,\n\u001b[1;32m    204\u001b[0m         pyFiles,\n\u001b[1;32m    205\u001b[0m         environment,\n\u001b[1;32m    206\u001b[0m         batchSize,\n\u001b[1;32m    207\u001b[0m         serializer,\n\u001b[1;32m    208\u001b[0m         conf,\n\u001b[1;32m    209\u001b[0m         jsc,\n\u001b[1;32m    210\u001b[0m         profiler_cls,\n\u001b[1;32m    211\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/context.py:287\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[1;32m    288\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:556)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Top3CountriesByAvgExport\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/revised_final_data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for \"Goods\" category and updated time_ref (2023-06-30)\n",
    "filtered_df = df.filter((col(\"product_type\") == \"Goods\") & (col(\"time_ref\") == \"2023-06-30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification to partition by month and order by average export value\n",
    "window_spec = Window.partitionBy(\"time_ref\").orderBy(col(\"avg_export_value\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average export value for each country in each month\n",
    "avg_export_df = (\n",
    "    filtered_df\n",
    "    .groupBy(\"time_ref\", \"country_code\")\n",
    "    .agg(avg(col(\"value\")).alias(\"avg_export_value\"))\n",
    "    .withColumn(\"rank\", rank().over(window_spec))\n",
    ")\n",
    "\n",
    "# Filter the top 3 countries for each month\n",
    "top3_countries_by_month = avg_export_df.filter(col(\"rank\") <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+----+\n",
      "|  time_ref| country_code|avg_export_value|rank|\n",
      "+----------+-------------+----------------+----+\n",
      "|2023-06-30|TOT (OMT CIF)| 1.9576711757E10|   1|\n",
      "|2023-06-30|TOT (OMT VFD)| 1.8547416973E10|   2|\n",
      "|2023-06-30|           DZ|    7.94222382E7|   3|\n",
      "+----------+-------------+----------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "top3_countries_by_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Calculate the total import value for each country in the 'revised_final' table. Show the top 10 countries with the highest total export values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, desc\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TradeAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the 'revised_final' table from your dataset\n",
    "revised_final = spark.read.csv(\"path_to_revised_final.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter the data for imports (account = \"Imports\") and time_ref = \"2023-06-30\"\n",
    "import_data = revised_final.filter((revised_final[\"account\"] == \"Imports\") & (revised_final[\"time_ref\"] == \"2023-06-30\"))\n",
    "\n",
    "# Calculate the total import value for each country\n",
    "total_imports = import_data.groupBy(\"country_code\").agg(sum(\"value\").alias(\"total_import_value\"))\n",
    "\n",
    "# Show the top 10 countries with the highest total import values\n",
    "top_10_importing_countries = total_imports.orderBy(desc(\"total_import_value\")).limit(10)\n",
    "\n",
    "# Show the output\n",
    "top_10_importing_countries.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, desc\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TradeAnalysis\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the 'revised_final' table from your dataset\n",
    "revised_final = spark.read.csv(\"/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Group_project/data/revised_final_data1.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for imports (account = \"Imports\") and time_ref = \"2023-06-30\"\n",
    "import_data = revised_final.filter((revised_final[\"account\"] == \"Imports\") & (revised_final[\"time_ref\"] == \"2023-06-30\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the total import value for each country\n",
    "total_imports = import_data.groupBy(\"country_code\").agg(sum(\"value\").alias(\"total_import_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 countries with the highest total import values\n",
    "top_10_importing_countries = total_imports.orderBy(desc(\"total_import_value\")).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "| country_code|total_import_value|\n",
      "+-------------+------------------+\n",
      "|TOT (OMT CIF)|   1.9576711757E10|\n",
      "|TOT (OMT VFD)|   1.8547416973E10|\n",
      "|           AU|   1.2831898537E10|\n",
      "|           CN|   1.1493681947E10|\n",
      "|           US|     9.651088289E9|\n",
      "|           SG|     4.870192374E9|\n",
      "|           JP|     4.309482252E9|\n",
      "|           KR|     3.588781722E9|\n",
      "|           GB|      2.65543796E9|\n",
      "|           MY|     2.601556325E9|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the output\n",
    "top_10_importing_countries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
